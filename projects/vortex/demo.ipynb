{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VORTEX\n",
    "**Brief:** Build an interactive application for testing different models under distribution shifts.\n",
    "\n",
    "Common acquisition-related perturbations in MRI, such as changes in signal-to-noise (SNR) and patient motion, can substantially degrade the quality of the reconstructed images.\n",
    "VORTEX helps mitigate this problem by building invariance to these perturbations during model training. VORTEX also helps reduce the amount of fully-sampled data required for training.\n",
    "\n",
    "In this demo, we will use [Meddlr](https://github.com/ad12/meddlr) and [Meerkat](https://github.com/HazyResearch/meerkat) to build interactive applications to explore how\n",
    "different models perform these two distribution shifts. We will learn how to:\n",
    "\n",
    "- Convert Meddlr datasets to Meerkat dataframes\n",
    "- Use pre-built interfaces in Meerkat to visualize our data\n",
    "- Integrate pre-trained or custom models into Meerkat\n",
    "\n",
    "**Reference:**\n",
    "    Desai et al. VORTEX: Physics-Driven Data Augmentations Using Consistency\n",
    "    Training for Robust Accelerated MRI Reconstruction. MIDL 2022.\n",
    "\n",
    "**Requirements:**\n",
    "- `pip install meerkat-ml meddlr meddlr-viz`\n",
    "- `pip install torch torchvision`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import meddlr as mr\n",
    "import meerkat as mk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Meerkat Server\n",
    "Starting the meerkat server will allow us to interact with the Meerkat application in the notebook.\n",
    "\n",
    "If you do not see a view at the bottom of the notebook, change the `api_port` and `frontend_port` and restart the notebook.\n",
    "\n",
    "**Remote Server:** If you are running this notebook on a remote machine, you will need to forward the api and frontend ports to your local machine:\n",
    "\n",
    "```bash\n",
    "# If api_port = 5000 and frontend_port = 8000\n",
    "ssh -L 5000:localhost:5000 -L 8000:localhost:8000 <user>@<remote-machine>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(APIInfo(api=<fastapi.applications.FastAPI object at 0x2ae060af0>, port=5000, server=<meerkat.interactive.server.Server object at 0x104aaf070>, name='127.0.0.1', shared=False, process=None, _url=None),\n",
       " FrontendInfo(package_manager='npm', port=8000, name='localhost', shared=False, process=<subprocess.Popen object at 0x2ad65ffd0>, _url=None))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mk.gui.start(api_port=5000, frontend_port=8000, dev=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build DataFrame\n",
    "Meerkat DataFrames help manage complex data types, such as high dimensional images, kspace, etc.\n",
    "\n",
    "Let's convert the mridata Stanford 3D knee FSE test split into a Meerkat DataFrame.\n",
    "The dataset is in the ismrmrd HDF5 format, with an additional field for sensitivity maps (`maps`).\n",
    "\n",
    "Each row in the dataframe will correspond to of axial slices of scans from the knee dataset.\n",
    "Columns will include:\n",
    "\n",
    "- `kspace`: The full-sampled kspace for the `ky x kz` slice\n",
    "- `target`: The ground truth slice\n",
    "- `maps`: The sensitivity maps for the slice\n",
    "\n",
    "**Note:** If you have the dataset downloaded locally with `meddlr`, you can fetch the dataset using `DatasetCatalog`.\n",
    "\n",
    "This download may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from meddlr.data import DatasetCatalog\n",
    "\n",
    "# mridata Stanford 3D FSE dataset.\n",
    "paths = [\n",
    "    \"https://huggingface.co/datasets/arjundd/mridata-stanford-knee-3d-fse/resolve/main/files/ec00945c-ad90-46b7-8c38-a69e9e801074.h5\",\n",
    "    # Uncomment the following lines to load more data.\n",
    "    # \"https://huggingface.co/datasets/arjundd/mridata-stanford-knee-3d-fse/resolve/main/files/ee2efe48-1e9d-480e-9364-e53db01532d4.h5\",\n",
    "    # \"https://huggingface.co/datasets/arjundd/mridata-stanford-knee-3d-fse/resolve/main/files/efa383b6-9446-438a-9901-1fe951653dbd.h5\",\n",
    "]\n",
    "\n",
    "# If you have the Stanford 3D FSE dataset downloaded locally, you can use this:\n",
    "# dataset_dicts = DatasetCatalog.get(\"mridata_knee_2019_test\")\n",
    "# paths = [d[\"file_name\"] for d in dataset_dicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arjundd/miniconda3/envs/meerkat_prod/lib/python3.8/site-packages/meerkat/ops/map.py:260: UserWarning: Non-default argument 'row' does not have a corresponding column in the DataFrame. If your function expects a full DataFrame row, pass ``inputs='row'`` to ``map``. Otherwise, please provide an `inputs` mapping or pass a lambda function with a different signature. See map documentation for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Convert paths to the slice dataframe.\n",
    "from meddlr_viz.utils import build_slice_df\n",
    "\n",
    "df = build_slice_df(paths, defer=True)\n",
    "df[\"id\"] = df[\"path\"].apply(lambda x: x.split(\"/\")[-1].split(\".\")[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction Models\n",
    "Meddlr offers several reconstruction models in its Model Zoo and easy-to-use [APIs](https://github.com/ad12/meddlr#-model-zoo).\n",
    "\n",
    "Let's start by using a few pre-trained models from the VORTEX paper. Let's compare these models:\n",
    "- `Supervised`: A supervised model trained on fully-sampled data\n",
    "- `Supervised + Aug`: A supervised model trained on fully-sampled data with physics-based augmentations\n",
    "- [`SSDU`](https://onlinelibrary.wiley.com/doi/abs/10.1002/mrm.28378): Self-supervised via data undersampling trained with both fully-sampled and undersampled scans\n",
    "- [`VORTEX`](https://arxiv.org/abs/2111.02549): Trained with both fully-sampled and undersampled scans\n",
    "\n",
    "These models are hosted on huggingface in the Meddlr format.\n",
    "Providing the urls for these models will automatically download and load them in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained models from the VORTEX paper.\n",
    "# More pre-trained models are available at https://github.com/ad12/meddlr/blob/main/projects/vortex/MODEL_ZOO.md\n",
    "MODELS = {\n",
    "    \"Supervised\": \"https://huggingface.co/arjundd/noise2recon-release/resolve/main/mridata_knee_3dfse/12x/Supervised_1sub\",\n",
    "    \"Supervised + Aug\": \"https://huggingface.co/arjundd/vortex-release/resolve/main/mridata_knee_3dfse/Aug_Physics\",\n",
    "    \"SSDU\": \"https://huggingface.co/arjundd/vortex-release/resolve/main/mridata_knee_3dfse/SSDU\",\n",
    "    \"VORTEX\": \"https://huggingface.co/arjundd/vortex-release/resolve/main/mridata_knee_3dfse/VORTEX_Physics\",\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: Adding your own models\n",
    "\n",
    "Interested in using your own models? No problem! Just write a wrapper module for your model.\n",
    "\n",
    "Let's make a dummy model that takes in kspace and returns the zero-filled reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from typing import Dict\n",
    "from meddlr.forward.mri import SenseModel\n",
    "\n",
    "class ZeroFilledModel(nn.Module):\n",
    "    def forward(self, inputs: Dict[str, torch.Tensor]):\n",
    "        \"\"\"\"\"\"\n",
    "        A = SenseModel(inputs[\"maps\"], weights=inputs[\"mask\"])\n",
    "        return A(inputs[\"kspace\"], adjoint=True)\n",
    "\n",
    "MODELS[\"Dummy Model\"] = ZeroFilledModel()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `MRIPerturbationInference` Interface\n",
    "\n",
    "We added `MRIPerturbationInference` to Meerkat's suite of pre-build interfaces.\n",
    "\n",
    "In this interface we can:\n",
    "- Interactively control the SNR, 1D translational motion extent, and acceleration we apply to the k-space\n",
    "- Toggle what models we want to test\n",
    "- Change the scans that we want to visualize\n",
    "\n",
    "This interface gives a quick way to visualize results from your models without having the overhead of writing the scans to disk.\n",
    "\n",
    "**Note:** All reconstructions are computed dynamically. If you are using a CPU, this may take a while. If you have access to a GPU, we recommend using it for this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"600px\"\n",
       "            src=\"http://localhost:8000/?id=MRIPerturbationInferenceae9061ea-bf80-423b-b07b-aee13737193f\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2cd21a7c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arjundd/miniconda3/envs/meerkat_prod/lib/python3.8/site-packages/meddlr/modeling/meta_arch/ssdu.py:143: UserWarning: Edge mask not found in `inputs`. Assuming no edge mask.\n",
      "  warnings.warn(\"Edge mask not found in `inputs`. Assuming no edge mask.\")\n",
      "/Users/arjundd/miniconda3/envs/meerkat_prod/lib/python3.8/site-packages/meddlr/modeling/meta_arch/ssdu.py:143: UserWarning: Edge mask not found in `inputs`. Assuming no edge mask.\n",
      "  warnings.warn(\"Edge mask not found in `inputs`. Assuming no edge mask.\")\n",
      "/Users/arjundd/miniconda3/envs/meerkat_prod/lib/python3.8/site-packages/meddlr/modeling/meta_arch/ssdu.py:143: UserWarning: Edge mask not found in `inputs`. Assuming no edge mask.\n",
      "  warnings.warn(\"Edge mask not found in `inputs`. Assuming no edge mask.\")\n"
     ]
    }
   ],
   "source": [
    "from meddlr_viz.gui.perturbation import MRIPerturbationInference\n",
    "\n",
    "view = MRIPerturbationInference(df, models=MODELS, acc=(12, 24, 1))\n",
    "view._get_ipython_height = lambda: \"600px\"\n",
    "\n",
    "view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meddlr_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
